{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loan Default Prediction\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "The objective of this project is to build a predictive model that can accurately determine whether a borrower will default on a loan or not. Using historical loan data, we aim to analyze the key factors influencing loan defaults and develop a machine learning model to predict the binary target variable: **Default (1: Defaulted, 0: Not Defaulted)**.\n",
    "\n",
    "- **Loan_ID**: Unique Loan ID\n",
    "- **Gender**: Male/ Female\n",
    "- **Married**: Applicant married (Y/N)\n",
    "- **Dependents**: Number of dependents\n",
    "- **Education**: Applicant Education (Graduate/Under Graduate)\n",
    "- **Self_Employed**: Self employed (Y/N)\n",
    "- **ApplicantIncome**: Applicant income\n",
    "- **CoapplicantIncome**: Coapplicant income\n",
    "- **LoanAmount**: Loan amount in thousands\n",
    "- **Loan_Amount_Term**: Term of loan in months\n",
    "- **Credit_History**: Credit history meets guidelines\n",
    "- **Property_Area**: Urban/ Semi Urban/ Rural\n",
    "- **Loan_Status**: Loan approved (Y/N)\n",
    "\n",
    "## Project Goals\n",
    "\n",
    "1. **Data Preprocessing**:\n",
    "    - Understand dataset\n",
    "    - Handle missing values and outliers.\n",
    "    - Convert categorical features into numerical representations.\n",
    "    - Standardize numerical features for consistent model performance.\n",
    "    - Explore correlations among features to understand relationships.\n",
    "\n",
    "2. **Exploratory Data Analysis (EDA)**:\n",
    "    - Visualize the distribution of key features such as `Income`, `LoanAmount`, and `CreditScore`.\n",
    "    - Explore the relationship between features and loan default.\n",
    "    - Examine the class distribution of the target variable (imbalanced or not).\n",
    "\n",
    "3. **Feature Engineering**:\n",
    "    - Engineer new features that may contribute to better prediction accuracy (e.g., `Debt-to-Income Ratio` buckets).\n",
    "    - Perform feature selection based on correlation analysis, domain knowledge, and model performance.\n",
    "\n",
    "4. **Modeling**:\n",
    "    - Experiment with multiple machine learning models such as:\n",
    "        - Logistic Regression\n",
    "        - Decision Trees\n",
    "        - Random Forest\n",
    "        - Gradient Boosting Machines (GBM)\n",
    "        - Support Vector Machines (SVM)\n",
    "    - Tune hyperparameters to optimize performance using cross-validation.\n",
    "\n",
    "5. **Evaluation Metrics**:\n",
    "    - Measure model performance using metrics such as:\n",
    "        - Accuracy\n",
    "        - Precision\n",
    "        - Recall\n",
    "        - F1-score\n",
    "        - ROC-AUC (Receiver Operating Characteristic - Area Under Curve)\n",
    "    - Handle any class imbalance using techniques like SMOTE (Synthetic Minority Over-sampling Technique) or class weighting.\n",
    "\n",
    "## Expected Outcomes\n",
    "\n",
    "The project aims to deliver a predictive model with a high degree of accuracy in identifying potential loan defaulters. The model will help financial institutions assess borrower risk more effectively, reducing default rates and improving decision-making processes.\n",
    "\n",
    "## Tools and Technologies\n",
    "\n",
    "- **Python**: For data analysis and model development.\n",
    "- **Pandas**: For data manipulation and analysis.\n",
    "- **NumPy**: For numerical computations.\n",
    "- **Matplotlib/Seaborn**: For data visualization.\n",
    "- **Scikit-learn**: For model development, evaluation, and tuning.\n",
    "- **Imbalanced-learn (if needed)**: For dealing with class imbalance.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "By leveraging machine learning models on borrower data, this notebook will predict the likelihood of a borrower defaulting on a loan. Insights gained from this analysis will provide actionable intelligence to stakeholders in the lending space, potentially reducing financial losses due to loan defaults.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "\n",
    "\n",
    "#### Understanding the Dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Or to suppress specific warnings (e.g., FutureWarnings)\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/train.csv\")\n",
    "test = pd.read_csv(\"data/test.csv\")\n",
    "original_data = data.copy()\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Values Analysis\n",
    "\n",
    "Analyzing missing values in a dataset is crucial for several reasons:\n",
    "\n",
    "- **Data Quality**: Identifies columns with missing data, which may affect the quality and reliability of the analysis or model.\n",
    "- **Data Cleaning**: Helps determine the extent of missing data, guiding decisions on imputation, removal, or other data cleaning methods.\n",
    "- **Understanding Impact**: Provides insights into the data's completeness, allowing for better interpretation of results and ensuring that conclusions drawn are based on a robust dataset.\n",
    "- **Model Performance**: Missing values can impact model performance; understanding their distribution helps in applying appropriate preprocessing techniques.\n",
    "\n",
    "By calculating both the count and percentage of missing values, you gain a clearer picture of the dataset’s integrity and can make informed decisions on how to handle the missing data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate missing values and percentage\n",
    "missing_values = data.isnull().sum()\n",
    "missing_percentage = (missing_values / len(data)) * 100\n",
    "\n",
    "# Round the percentages and format as strings\n",
    "missing_summary = pd.DataFrame(\n",
    "    {\n",
    "        \"Missing Values\": missing_values,\n",
    "        \"Percentage\": missing_percentage.round(2).astype(str) + \"%\",\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Missing Data Summary:\")\n",
    "display(missing_summary)\n",
    "\n",
    "# Identify categorical and numerical features\n",
    "categorical_features_train = data.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "categorical_features_test = test.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "\n",
    "numerical_features = data.select_dtypes(include=[\"float64\", \"int64\"]).columns.tolist()\n",
    "\n",
    "# Impute categorical features with mode\n",
    "imputer_cat = SimpleImputer(strategy=\"most_frequent\")\n",
    "data[categorical_features_train] = imputer_cat.fit_transform(\n",
    "    data[categorical_features_train]\n",
    ")\n",
    "\n",
    "# Impute numerical features with median\n",
    "imputer_num = SimpleImputer(strategy=\"median\")\n",
    "data[numerical_features] = imputer_num.fit_transform(data[numerical_features])\n",
    "\n",
    "# Verify missing values after imputation\n",
    "missing_values_after = data.isnull().sum()\n",
    "missing_percentage_after = (missing_values_after / len(data)) * 100\n",
    "\n",
    "# Round the percentages and format as strings\n",
    "missing_summary_after = pd.DataFrame(\n",
    "    {\n",
    "        \"Missing Values\": missing_values_after,\n",
    "        \"Percentage\": missing_percentage_after.round(2).astype(str) + \"%\",\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"\\nMissing Data Summary After Imputation:\")\n",
    "display(missing_summary_after)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate Analysis\n",
    "Perform univariate analysis by examining each variable individually. For categorical features, frequency tables or bar plots visualize category counts. For numerical variables, Probability Density Functions (PDFs) reveal their distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Target Variable\n",
    "\n",
    "The target variable, Default, is a categorical variable. A frequency table, percentage distribution, and bar plot will be used to analyze it. The frequency table provides the count of each category within the variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set global plot style\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams[\"axes.titlesize\"] = 20  # Title font size\n",
    "plt.rcParams[\"legend.fontsize\"] = 15  # Legend font size\n",
    "palette = [\n",
    "    \"#00bcd4\",\n",
    "    \"#4169e1\",\n",
    "    \"#003366\",\n",
    "    \"#4682b4\",\n",
    "]  # Cyan, Royal Blue, Navy Blue, Steel Blue\n",
    "\n",
    "sns.countplot(x=\"Loan_Status\", data=data, palette=palette)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Univariate Analysis - Categorical Variables\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 12))\n",
    "\n",
    "# Nominal variables\n",
    "sns.countplot(data=data, x=\"Gender\", ax=axes[0, 0], palette=palette)\n",
    "axes[0, 0].set_title(\"Gender Distribution\")\n",
    "\n",
    "sns.countplot(data=data, x=\"Married\", ax=axes[0, 1], palette=palette)\n",
    "axes[0, 1].set_title(\"Marriage Status Distribution\")\n",
    "\n",
    "sns.countplot(data=data, x=\"Dependents\", ax=axes[0, 2], palette=palette)\n",
    "axes[0, 2].set_title(\"Dependents Number Distribution\")\n",
    "\n",
    "# Binary variables\n",
    "sns.countplot(data=data, x=\"Education\", ax=axes[1, 0], palette=palette)\n",
    "axes[1, 0].set_title(\"Education Distribution\")\n",
    "\n",
    "sns.countplot(data=data, x=\"Self_Employed\", ax=axes[1, 1], palette=palette)\n",
    "axes[1, 1].set_title(\"Self_Employed status Distribution\")\n",
    "\n",
    "sns.countplot(data=data, x=\"Property_Area\", ax=axes[1, 2], palette=palette)\n",
    "axes[1, 2].set_title(\"Property Area Distribution\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[\"Education\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate crosstabs\n",
    "Education = pd.crosstab(data[\"Education\"], data[\"Loan_Status\"])\n",
    "Married = pd.crosstab(data[\"Married\"], data[\"Loan_Status\"])\n",
    "Dependents = pd.crosstab(data[\"Dependents\"], data[\"Loan_Status\"])\n",
    "Self_Employed = pd.crosstab(data[\"Self_Employed\"], data[\"Loan_Status\"])\n",
    "Gender = pd.crosstab(data[\"Gender\"], data[\"Loan_Status\"])\n",
    "Property_Area = pd.crosstab(data[\"Property_Area\"], data[\"Loan_Status\"])\n",
    "\n",
    "# Define the number of rows and columns for the plot grid\n",
    "num_rows = 2\n",
    "num_columns = 3\n",
    "\n",
    "# Create subplots with the updated number of rows and columns\n",
    "fig, axes = plt.subplots(\n",
    "    num_rows, num_columns, figsize=(15, 12), constrained_layout=True\n",
    ")\n",
    "\n",
    "# Define the custom color palette\n",
    "colors = [\"#003366\", \"#3399FF\"]  # Navy blue and normal blue\n",
    "\n",
    "# Plot each cross-tabulation in specified subplot\n",
    "Education.div(Education.sum(1).astype(float), axis=0).plot(\n",
    "    kind=\"bar\", stacked=True, ax=axes[0, 0], color=colors\n",
    ")\n",
    "axes[0, 0].set_title(\"Education Distribution\")\n",
    "axes[0, 0].set_xlabel(\"Education\")\n",
    "axes[0, 0].set_ylabel(\"Proportion\")\n",
    "\n",
    "Married.div(Married.sum(1).astype(float), axis=0).plot(\n",
    "    kind=\"bar\", stacked=True, ax=axes[0, 1], color=colors\n",
    ")\n",
    "axes[0, 1].set_title(\"Marital Status Distribution\")\n",
    "axes[0, 1].set_xlabel(\"Marital Status\")\n",
    "axes[0, 1].set_ylabel(\"Proportion\")\n",
    "\n",
    "Dependents.div(Dependents.sum(1).astype(float), axis=0).plot(\n",
    "    kind=\"bar\", stacked=True, ax=axes[0, 2], color=colors\n",
    ")\n",
    "axes[0, 2].set_title(\"Dependents Distribution\")\n",
    "axes[0, 2].set_xlabel(\"Dependents\")\n",
    "axes[0, 2].set_ylabel(\"Proportion\")\n",
    "\n",
    "Self_Employed.div(Self_Employed.sum(1).astype(float), axis=0).plot(\n",
    "    kind=\"bar\", stacked=True, ax=axes[1, 0], color=colors\n",
    ")\n",
    "axes[1, 0].set_title(\"Self_Employed Status Distribution\")\n",
    "axes[1, 0].set_xlabel(\"Self Employed Status\")\n",
    "axes[1, 0].set_ylabel(\"Proportion\")\n",
    "\n",
    "Gender.div(Gender.sum(1).astype(float), axis=0).plot(\n",
    "    kind=\"bar\", stacked=True, ax=axes[1, 1], color=colors\n",
    ")\n",
    "axes[1, 1].set_title(\"Gender Distribution\")\n",
    "axes[1, 1].set_xlabel(\"Gender\")\n",
    "axes[1, 1].set_ylabel(\"Proportion\")\n",
    "\n",
    "Property_Area.div(Property_Area.sum(1).astype(float), axis=0).plot(\n",
    "    kind=\"bar\", stacked=True, ax=axes[1, 2], color=colors\n",
    ")\n",
    "axes[1, 2].set_title(\"Property Area Distribution\")\n",
    "axes[1, 2].set_xlabel(\"Property Area\")\n",
    "axes[1, 2].set_ylabel(\"Proportion\")\n",
    "\n",
    "# Remove any unused subplots\n",
    "# Since there are only 6 plots and we have a 2x3 grid, all subplots are used in this case.\n",
    "# Ensure no unused subplot remains by deleting extra subplot axes if they exist\n",
    "for ax in axes.flat:\n",
    "    if not ax.has_data():  # Check if the axis has data; if not, remove it\n",
    "        fig.delaxes(ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical Analysis\n",
    "\n",
    "In this section, various plots are used to explore and understand the numerical features of the dataset.\n",
    "\n",
    "- **Histograms** reveal the distribution and frequency of individual numerical variables.\n",
    "- **Box Plots** provide insights into the spread, central tendency, and potential outliers within numerical features.\n",
    "- **Pair Plots** examine the relationships between pairs of numerical features to identify correlations and interactions.\n",
    "- **Correlation Heatmaps** visualize the strength and direction of linear relationships between numerical features.\n",
    "\n",
    "### Analysis Type\n",
    "\n",
    "The combination of these plots for numerical features extends beyond univariate analysis to include **bivariate** and **multivariate analysis**. Bivariate analysis examines the relationships between two variables, while multivariate analysis explores interactions among multiple variables simultaneously. This holistic approach provides a deeper understanding of the dataset's structure and relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical features\n",
    "numerical_features = data.select_dtypes(include=[\"float64\", \"int64\"]).columns.tolist()\n",
    "\n",
    "# Initialize StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the numerical features\n",
    "numeric_scaled = scaler.fit_transform(data[numerical_features])\n",
    "\n",
    "# Convert back to a DataFrame\n",
    "numeric_scaled_df = pd.DataFrame(numeric_scaled, columns=numerical_features)\n",
    "\n",
    "# Plot histograms for scaled features\n",
    "fig, axes = plt.subplots(\n",
    "    len(numerical_features), 1, figsize=(12, len(numerical_features) * 4)\n",
    ")\n",
    "\n",
    "for i, feature in enumerate(numerical_features):\n",
    "    sns.histplot(\n",
    "        numeric_scaled_df[feature], bins=20, kde=True, ax=axes[i], color=\"blue\"\n",
    "    )\n",
    "    axes[i].set_title(f\"Distribution of Scaled {feature}\")\n",
    "    axes[i].set_xlabel(feature)\n",
    "    axes[i].set_ylabel(\"Frequency\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"ApplicantIncome_log\"] = np.log(data[\"ApplicantIncome\"])\n",
    "data[\"LoanAmount_log\"] = np.log(data[\"LoanAmount\"])\n",
    "data[\"CoapplicantIncome_log\"] = np.log(data[\"CoapplicantIncome\"])\n",
    "\n",
    "data[\"ApplicantIncome_log\"].hist(bins=20)\n",
    "plt.title(\"Histogram of Log-Transformed Applicant Income Distribution\")\n",
    "plt.show()\n",
    "data[\"LoanAmount_log\"].hist(bins=20)\n",
    "plt.title(\"Histogram of Log-Transformed Coapplicant Income Distribution\")\n",
    "plt.show()\n",
    "data[\"CoapplicantIncome\"].hist(bins=20)\n",
    "plt.show()\n",
    "\n",
    "# Drop original features if log-transformed features are preferable\n",
    "data.drop([\"ApplicantIncome\", \"LoanAmount\", \"CoapplicantIncome\"], axis=1, inplace=True)\n",
    "\n",
    "data[\"ApplicantIncome_log\"] = np.log(test[\"ApplicantIncome\"])\n",
    "data[\"LoanAmount_log\"] = np.log(test[\"LoanAmount\"])\n",
    "data[\"CoapplicantIncome_log\"] = np.log(test[\"CoapplicantIncome\"])\n",
    "\n",
    "test[\"ApplicantIncome_log\"] = np.log(test[\"ApplicantIncome\"])\n",
    "test[\"LoanAmount_log\"] = np.log(test[\"LoanAmount\"])\n",
    "test[\"CoapplicantIncome_log\"] = np.log(test[\"CoapplicantIncome\"])\n",
    "\n",
    "# Drop original features if log-transformed features are preferable\n",
    "test.drop([\"ApplicantIncome\", \"LoanAmount\", \"CoapplicantIncome\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Box Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = data.select_dtypes(include=[\"float64\", \"int64\"]).columns.tolist()\n",
    "\n",
    "# Box plots for numerical variables\n",
    "fig, axes = plt.subplots(\n",
    "    len(numerical_features), 1, figsize=(12, len(numerical_features) * 3)\n",
    ")\n",
    "for i, feature in enumerate(numerical_features):\n",
    "    sns.boxplot(x=data[feature], ax=axes[i], color=\"purple\")\n",
    "    axes[i].set_title(f\"Box Plot of {feature}\")\n",
    "    axes[i].set_xlabel(feature)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Pair Plots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pair plots for numerical variables\n",
    "sns.pairplot(data[numerical_features], diag_kind=\"kde\")\n",
    "plt.suptitle(\"Pair Plot of Numerical Features\", y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Correlation Heatmap\n",
    "\n",
    "\n",
    "Based on the heatmap below, here are some major deductions:\n",
    "\n",
    "- A moderate positive correlation of 0.46 between Applicant Income and Loan Amount means that as people’s incomes go up, they generally borrow more money.\n",
    "\n",
    "- A correlation of 0.19 between Coapplicant Income and Loan Amount shows a weak positive relationship, meaning that slightly higher coapplicant income is associated with a slightly larger loan amount, but the effect is minimal.\n",
    "\n",
    "- A correlation of 0.12 between Loan Amount Term and Coapplicant Income indicates a very weak positive relationship, suggesting that coapplicant income has only a minimal impact on the length of the loan term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute correlation matrix\n",
    "corr_matrix = data[numerical_features].corr()\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap=\"Blues\", fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Correlation Heatmap of Numerical Features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sample(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"CoapplicantIncome_log\"].replace(-np.inf, 0, inplace=True)\n",
    "data[\"ApplicantIncome_log\"].replace(-np.inf, 0, inplace=True)\n",
    "data[\"LoanAmount_log\"].replace(-np.inf, 0, inplace=True)\n",
    "\n",
    "test[\"CoapplicantIncome_log\"].replace(-np.inf, 0, inplace=True)\n",
    "test[\"ApplicantIncome_log\"].replace(-np.inf, 0, inplace=True)\n",
    "test[\"LoanAmount_log\"].replace(-np.inf, 0, inplace=True)\n",
    "\n",
    "\n",
    "# Impute categorical features with mode\n",
    "imputer_cat = SimpleImputer(strategy=\"most_frequent\")\n",
    "data[categorical_features_train] = imputer_cat.fit_transform(\n",
    "    data[categorical_features_train]\n",
    ")\n",
    "test[categorical_features_test] = imputer_cat.fit_transform(\n",
    "    test[categorical_features_test]\n",
    ")\n",
    "\n",
    "# Impute numerical features with median\n",
    "imputer_num = SimpleImputer(strategy=\"median\")\n",
    "data[numerical_features] = imputer_num.fit_transform(data[numerical_features])\n",
    "test[numerical_features] = imputer_num.fit_transform(test[numerical_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Convert all categorical variables to numeric for modeling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the target variable 'Loan_Status'\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(data[\"Loan_Status\"])\n",
    "\n",
    "# Drop 'Loan_ID' and 'Loan_Status' from the feature set\n",
    "X = data.drop([\"Loan_ID\", \"Loan_Status\"], axis=1)\n",
    "test_loan_id = test[\"Loan_ID\"]\n",
    "test = test.drop(\"Loan_ID\", axis=1)\n",
    "\n",
    "# Apply One-Hot Encoding to the categorical features\n",
    "X = pd.get_dummies(X)\n",
    "test = pd.get_dummies(test)\n",
    "\n",
    "# Check the sample data\n",
    "X.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting the dataset into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"SVC\": SVC(),\n",
    "    \"K-Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "    \"Gradient Boosting Classifier\": GradientBoostingClassifier(),\n",
    "}\n",
    "\n",
    "# Set up cross-validation\n",
    "cv = StratifiedKFold(n_splits=5)\n",
    "\n",
    "# Evaluate models using cross-validation\n",
    "results = {}\n",
    "for model_name, model in models.items():\n",
    "    cv_scores = cross_val_score(model, X, y, cv=cv, scoring=\"accuracy\")\n",
    "    results[model_name] = cv_scores\n",
    "    print(\n",
    "        f\"{model_name}: Mean Accuracy = {cv_scores.mean():.4f}, Std = {cv_scores.std():.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (accuracy_score, classification_report,\n",
    "                             confusion_matrix)\n",
    "\n",
    "# Initialize Logistic Regression model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "predictions = model.predict(test)\n",
    "\n",
    "# Create a DataFrame to hold predictions\n",
    "submission = pd.DataFrame(\n",
    "    {\n",
    "        \"Loan_ID\": test_loan_id,  # Include this if it's part of the submission format\n",
    "        \"Loan_Status\": predictions,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Save predictions to CSV for submission\n",
    "submission.to_csv(\"data/my_submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.head(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zindi_projects-Vt0mrh2a-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
